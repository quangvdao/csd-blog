+++
# The title of your blogpost. No sub-titles are allowed, nor are line-breaks.
title = "On Noisy Evaluation in Federated Hyperparameter Tuning"
# Date must be written in YYYY-MM-DD format. This should be updated right before the final PR is made.
date = 2025-10-28

[taxonomies]
# Keep any areas that apply, removing ones that don't. Do not add new areas!
areas = ["Artificial Intelligence", "Systems"]
# Tags can be set to a collection of a few keywords specific to your blogpost.
# Consider these similar to keywords specified for a research paper.
tags = ["distributed-learning", "federated-learning", "hyperparameter-tuning", "transfer-learning"]

[extra]
# For the author field, you can decide to not have a url.
# If so, simply replace the set of author fields with the name string.
# For example:
#   author = "Harry Bovik"
# However, adding a URL is strongly preferred
author = {name = "Kevin Kuo", url = "https://imkevinkuo.github.io" }
# The committee specification is simply a list of strings.
# However, you can also make an object with fields like in the author.
committee = [
    {name="Aditi Raghunathan", url="https://www.cs.cmu.edu/~aditirag"},
    {name="Graham Neubig", url="https://www.phontron.com"},
    {name="Hojin Park", url="https://hojinp.github.io"}
]
+++

Hyperparameter tuning is critical to the success of cross-device federated learning applications. Unfortunately, federated networks face issues of scale, heterogeneity, and privacy; addressing these issues with standard techniques (client subsampling and differential privacy) introduces noise in the tuning process and make it difficult to faithfully evaluate the performance of various hyperparameters. Our work ([MLSys’23](https://proceedings.mlsys.org/paper_files/paper/2023/hash/3796d87d44d7ee1d9fa2dc38377afa75-Abstract-mlsys2023.html)) explores key sources of noise and surprisingly shows that even small amounts of noise can significantly harm tuning methods--for instance, on CIFAR10, noisy evaluation reduces the performance of state-of-the-art BOHB from 70% to 10% accuracy, while Random Search only degrades from 65% to 60%. To address noisy evaluation in such scenarios, we propose a simple and effective approach that leverages public proxy data to boost the evaluation signal. Our work establishes general challenges, baselines, and best practices for future work in federated hyperparameter tuning.

# Federated Learning: An Overview
<figure>
    <img src="./fl_overview.png"
         alt="Overview of federated learning (FL)">
    <figcaption><b>Figure 1.</b> In federated learning (FL), user data remains on the device and only model updates are communicated. (Source: <a href="https://en.wikipedia.org/wiki/Federated_learning">Wikipedia</a>)</figcaption>
</figure>

<p></p>

Cross-device federated learning (FL) is a machine learning setting that considers training a model over a large heterogeneous network of devices such as mobile phones or wearables. Three key factors differentiate FL from traditional centralized learning and distributed learning:

**Scale.** Cross-device refers to FL settings with many clients with potentially limited local resources e.g. training a language model across hundreds to millions of mobile phones. These devices have various resource constraints, such as limited upload speed, number of local examples, or computational capability.

**Heterogeneity.** Traditional distributed ML assumes each worker/client has a random (identically distributed) sample of the training data. In contrast, in FL client datasets may be non-identically distributed, with each user’s data being generated by a distinct underlying distribution.

**Privacy.** FL offers a baseline level of privacy since raw user data remains local on each client. However, FL is still vulnerable to post-hoc attacks where the public output of the FL algorithm (e.g. a model or its hyperparameters) can be reverse-engineered and leak private user information. A common approach to mitigate such vulnerabilities is to use differential privacy, which aims to mask the contribution of each client. However, differential privacy introduces noise in the aggregate evaluation signal, which can make it difficult to effectively select models.

Prior studies and reports have shown that these issues significantly impact the **training** phase of FL. For example, [(Hard et al., 2019)](https://arxiv.org/pdf/1811.03604) report on a large-scale FL system that samples 100 to 500 clients per round out of a population of 1.5 million clients; because of the extreme subsampling factor (3000 to 15000), one training run takes 4-5 days. In terms of model performance, data and systems heterogeneity [(Li et al., 2020)](https://proceedings.mlsys.org/paper_files/paper/2020/hash/1f5fe83998a09396ebe6477d9475ba0c-Abstract.html) and differential privacy [(McMahan, Ramage, Talwar, and Zhang, 2017)](https://arxiv.org/abs/1710.06963) can degrade the performance of standard FL training algorithms to trivial or random guessing levels. 

While FL research has proposed a variety of methods to improve model training, evaluation practices are much less established. For example, [(Hard et al., 2019)](https://arxiv.org/pdf/1811.03604) briefly mentions potential challenges with evaluation (they sampled training and evaluation clients from the same pool) and hyperparameter tuning (they generated synthetic data to tune architectural hyperparameters). Therefore, our our work takes a unique perspective on the FL pipeline and investigates the above challenges (scale, heterogeneity, and privacy) in the context of **hyperparameter tuning**.

# Federated Hyperparameter Tuning
Appropriately selecting hyperparameters (HPs) is critical to training quality models in FL. Hyperparameters are user-specified parameters that dictate the process of model training such as the learning rate, local batch size, and number of clients sampled at each round. The problem of tuning HPs is general to machine learning (not just FL). Given an HP search space and search budget, HP tuning methods aim to find a configuration in the search space that optimizes some measure of quality within a constrained budget.

Let’s first look at an end-to-end FL pipeline in Figure 2 that considers both the processes of training and hyperparameter tuning. In cross-device FL, we split the clients into two pools for training and validation. Given a hyperparameter configuration (\\(\lambda_s\\), \\(\lambda_c\\))
, we train a model using the training clients (explained in section “FL Training”). We then evaluate this model on the validation clients, obtaining an error rate/accuracy metric. We can then use the error rate to adjust the hyperparameters and train a new model.

<figure>
    <img src="./fl_hp_tuning.png"
         alt="FL HP Tuning">
    <figcaption><b>Figure 2.</b> A standard pipeline for tuning hyperparameters in cross-device FL.</figcaption>
</figure>

## Federated Training
A typical FL algorithm consists of several rounds of training where each client performs local training followed by aggregation of the client updates. In our work, we experiment with a general framework called FedOPT which was presented in [Adaptive Federated Optimization (Reddi et al. 2021)](https://openreview.net/forum?id=LkFG3lB13U5). We outline the per-round procedure of FedOPT:
1. The server broadcasts the model \\(\theta\\) to a sampled subset of \\(K\\)  clients.
2. Each client (in parallel) runs multiple steps of `ClientOPT` to train \\(\theta\\) on their local data \\(X_k\\), obtaining an updated model \\(\theta_k\\).
3. Each client sends \\(\theta_k\\) back to the server.
4. The server averages all the received models \\(\theta_k\\)
5. To update \\(\theta\\), the server computes the difference \\(\theta\\) - \\(\theta'\\) and treats this as a pseudo-gradient for one step of `ServerOPT`.

<figure>
    <img src="./fl_training.png"
         alt="Overview of federated training.">
    <figcaption><b>Figure 3.</b> The FedOPT framework and the five hyperparameters we consider tuning. (Source: edited from Wikipedia)</figcaption>
</figure>

Steps 2 and 5 of FedOPT each require a gradient-based optimization algorithm (called `ClientOPT` and `ServerOPT`) which specify how to update 
\\(\theta\\) given some update vector. In our work, we focus on an instantiation of FedOPT called FedAdam, which uses [Adam (Kingma and Ba 2014)](https://arxiv.org/abs/1412.6980) as ServerOPT and SGD as `ClientOPT`. We focus on tuning five hyperparameters: for SGD, we tune the learning rate and batch size, and for Adam, we tune the learning rate, 1st-moment decay, and 2nd-moment decay.

## Federated Evaluation
Now, we discuss how FL settings introduce noise to model evaluation. Consider the following example: We have \\(K=4\\) configurations (grey, blue, red, green) and we want to figure out which configuration has the best average accuracy across \\(N=5\\) clients. More specifically, each “configuration” is a set of HP values (learning rate, batch size, etc.) that are fed into an FL training algorithm (more details in the next section). This produces a model we can evaluate. If we can evaluate every model on every client then our evaluation is noiseless. In this case, we would be able to accurately determine that the green model performs the best. However, generating all the evaluations (as shown in Figure 4) is not practical, as evaluation costs scale with both the number of configurations and clients.

<figure>
    <img src="./fl_evaluation.png"
         alt="Overview of federated evaluation.">
    <figcaption><b>Figure 4.</b> Ideal HP tuning where every configuration is evaluated on every client. This allows us to find the best (green) configuration.
</figcaption>
</figure>

In Figure 5, we show an evaluation procedure that is more realistic in FL. As the primary challenge in cross-device FL is scale, we evaluate models using only a random subsample of clients. This is shown by the red ‘X’s and shaded-out phones. We cover three additional sources of noise in FL which can negatively interact with subsampling and introduce even more noise into the evaluation procedure:

**Data heterogeneity.** FL clients have non-identically distributed data, meaning that client evaluations can vary significantly on the same model. This is shown by the different histograms next to each client. Data heterogeneity is intrinsic to FL and is critical for our observations on noisy evaluation; if all clients had identical datasets, there would be no need to sample more than one client. A commonly studied example is [label heterogeneity (Hsu, Qi, amd Brown, 2019)](https://arxiv.org/abs/1909.06335), where the label frequencies vary across clients. In addition to this form of heterogeneity, our work also explores naturally occuring heterogeneity in user data from Reddit and StackOverflow.

**Systems heterogeneity.** In addition to data heterogeneity, clients may have heterogeneous system capabilities. For example, some clients have better network reception and computational hardware, which allows them to participate in training and evaluation more frequently. This biases performance towards these clients, leading to a poor overall model.

**Differential privacy.** Using the evaluation output (i.e. the top-performing model), a malicious party can infer whether or not a particular client participated in the FL procedure. At a high level, differential privacy aims to mask user contributions by adding noise to the aggregate evaluation metric. However, this additional noise can make it difficult to faithfully evaluate HP configurations.

<figure>
    <img src="./fl_evaluation_noisy.png"
         alt="Noisy federated evaluation.">
    <figcaption><b>Figure 5.</b> Evaluations can lead to suboptimal model selection when there is noise introduced by (1) client subsampling, (2) data heterogeneity, and (3) differential privacy. The combination of all these factors leads us to incorrectly choose the red model over the green one.
</figcaption>
</figure>

# Experimental Results
The first goal of our work is to investigate the impact of four sources of noisy evaluation that we outlined in the section “FL Evaluation”. In more detail, these are our research questions:

1. How does subsampling validation clients affect HP tuning performance?
2. How do the following factors interact with/exacerbate issues of subsampling?
  a. **data heterogeneity** (shuffling validation clients’ datasets)
  b. **systems heterogeneity** (biased client subsampling)
  c. **privacy** (adding Laplace noise to the aggregate evaluation)
3. In noisy settings, how do SOTA methods compare to simple baselines?

Surprisingly, we show that state-of-the-art HP tuning methods can perform catastrophically poorly, even worse than simple baselines (e.g., random search). While we only show results for CIFAR10, results on three other datasets (FEMNIST, StackOverflow, and Reddit) can be found in our paper.

## Noise hurts random search
This section investigates questions 1 and 2 using **random search (RS)** as the hyperparameter tuning method. RS is a simple baseline that *randomly samples* several HP configurations, trains a model for each one, and returns the highest-performing model. Generally, each hyperparameter value is sampled from a (log) uniform or normal distribution.

**Client subsampling.** We run RS while varying the client subsampling rate from a single client to the full validation client pool. “Best HPs” indicates the best HPs found across all trials of RS. **As we subsample less clients (left), random search performs worse (higher error rate).**

**Data heterogeneity.** We run RS on three separate validation partitions with varying degrees of data heterogeneity based on the label distributions on each client. **Client subsampling generally harms performance but has a greater impact on performance when the data is heterogeneous (IID Fraction = 0 vs. 1).**

<figure>
    <img src="./exps_1_2.png"
         alt="Subsampling and data heterogeneity">
    <figcaption><b>Figure 6.</b> Random search with varying only client subsampling (left) and varying both client subsampling and data heterogeneity (right). Each shaded band shows the min/max from 100 trials of RS (each trial samples 16 configurations.)
</figcaption>
</figure>

**Systems heterogeneity.** We run RS and bias the client sampling to reflect four degrees of systems heterogeneity. Based on the model that is currently being evaluated, we assign a higher probability of sampling clients who perform well on this model. **Sampling bias leads to worse performance since the biased evaluations are overly optimistic and do not reflect performance over the entire validation pool.**

**Privacy.** We run RS with 5 different evaluation privacy budgets \\(\epsilon\\). We add noise sampled from a Laplace distribution \\(Lap(M/(\epsilon|S|))\\) to the aggregate evaluation, where M is the number of evaluations (16), and \\(|S|\\) is the number of clients sampled for an evaluation (x-axis). **A smaller privacy budget requires sampling a larger raw number of clients to achieve reasonable performance.**

<figure>
    <img src="./exps_3_4.png"
         alt="Systems heterogeneity and privacy">
    <figcaption><b>Figure 7.</b> Random search with varying systems heterogeneity (left) and privacy budget (right). Both factors interact negatively with client subsampling.
</figcaption>
</figure>

## Noise hurts complex methods more than RS
Seeing that noise adversely affects random search, we now focus on question 3: Do the same observations hold for more complex tuning methods? In this experiment, we compare 4 representative HP tuning methods.

- [Random Search (RS)](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf?ref=blog.floydhub.com) is a naive baseline.
- [Tree-Structured Parzen Estimator (TPE)](https://proceedings.neurips.cc/paper_files/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html) is a selection-based method. These methods build a surrogate model that predicts the performance of various hyperparameters rather than predictions for the task at hand (e.g. image or language data).
- [Hyperband (HB)](https://www.jmlr.org/papers/volume18/16-558/16-558.pdf) is an allocation-based method. These methods allocate more resources to the most promising configurations. Hyperband initially samples a large number of configurations but stops training most of them after the first few rounds.
- [Bayesian Optimization + Hyperband (BOHB)](https://proceedings.mlr.press/v80/falkner18a.html) is a combined method that uses both the sampling strategy of TPE and the partial evaluations of HB.

<figure>
    <img src="./hpo_methods.png"
         alt="hyperparameter optimization methods">
    <figcaption><b>Figure 8.</b> Examples of (a) selection-based and (b) allocation-based HP tuning methods. (a) uses a surrogate model of the search space to sample the next configuration (numbered in order of exploration), while (b) randomly samples many configurations and adaptively allocates resources to the most promising ones. (Source: <a href="https://www.jmlr.org/papers/volume18/16-558/16-558.pdf">Hyperband (Li et al. 2018)</a>)
</figcaption>
</figure>

We report the error rate of each HP tuning method (y-axis) at a given budget of rounds (x-axis). **Surprisingly, we find that the relative ranking of these methods can be reversed when evaluation is noisy**. With noise, the performance of all methods degrades, but the degradation is particularly extreme for HB and BOHB. Intuitively, this is because these two methods already inject noise into the HP tuning procedure via early stopping which interacts poorly with additional sources of noise. Therefore, these results indicate a need for HP tuning methods that are specialized for FL, as many of the guiding principles for traditional hyperparameter tuning may not be effective at handling noisy evaluation in FL.

<figure>
    <img src="./hpo_noisy.png"
         alt="hpo with noise">
    <figcaption><b>Figure 9.</b> We compare 4 HP tuning methods in noiseless vs. noisy FL settings. In the noiseless setting (left), we always sample all the validation clients and do not consider privacy. In the noisy setting (right), we sample 1% of validation clients and have a generous privacy budget of epsilon=100.
</figcaption>
</figure>

## Proxy evaluation outperforms noisy evaluation
In practical FL settings, a practitioner may have access to **proxy data** i.e. public data at the server which is similar to the clients' data. In such settings, practitioners can run FL simulations to find the best hyperparameters on the proxy data; ideally, these hyperparameters provide a default setting which significalty reduces the need for additional tuning on the clients' private data. Still, given two distinct datasets, it is unclear how well hyperparameters can transfer between them. First, we explore the effectiveness of hyperparameter transfer between four datasets. In Figure 10, we see that the CIFAR10-FEMNIST and StackOverflow-Reddit pairs (top left, bottom right) show the clearest transfer between the two datasets. One likely reason for this is that these task pairs use the same model architecture: CIFAR10 and FEMNIST are both image classification tasks while StackOverflow and Reddit are next-word prediction tasks.

<figure>
    <img src="./hpo_proxy_transfer.png"
         alt="hpo cross-dataset transfer">
    <figcaption><b>Figure 10.</b> We experimented with 4 datasets in our work (CIFAR10, FEMNIST, StackOverflow, and Reddit). For each pair of datasets, we randomly sample 128 configurations and plot each configuration at the coordinates corresponding to the error rate on the two datasets.
</figcaption>
</figure>

Given the appropriate proxy dataset, we show that a simple method called "one-shot proxy random search" can perform extremely well. The algorithm has two steps:

1. Run a random search using *proxy data* to both train models and evaluate HPs. We assume the proxy data is both public and server-side, so we can always evaluate HPs without subsampling clients or adding privacy noise.
2. The output configuration from 1. is used to train a model on the training client data. Since we pass only a single configuration to this step, validation client data does not affect hyperparameter selection at all.

In each experiment, we choose one of these datasets to be partitioned among the clients and use the other three datasets as server-side proxy datasets. **Our results show that proxy data can be an effective solution, even if the proxy dataset is not an ideal match for the private data.** For example, on FEMNIST, optimized non-private hyperparameters (\\(\epsilon=\text{inf}\\)) performs the best, but the next-best performing configurations are all found via proxy data.

<figure>
    <img src="./hpo_proxy_noisy.png"
         alt="hpo cross-dataset transfer">
    <figcaption><b>Figure 11.</b> We compare tuning HPs using noisy evaluations on the private dataset (with 1% client subsampling and varying the privacy budget \\(\epsilon\\)
 versus noiseless evaluations on the proxy dataset. The proxy HP tuning methods appear as horizontal lines because they are one-shot.
</figcaption>
</figure>

# Conclusion

In conclusion, our study suggests several best practices for federated HP tuning:

1. Use simple HP tuning methods.
2. Sample a sufficiently large number of validation clients.
3. Evaluate a representative set of clients.
4. If available, proxy data can be an effective solution.

Furthermore, we identify several directions for future work in federated HP tuning:

- **Tailoring HP tuning methods for differential privacy and FL.** Early stopping methods are inherently noisy/biased and the large number of evaluations they use is at odds with privacy. Another useful direction is to investigate HP methods specific to noisy evaluation.
- **More detailed cost evaluation.** In our work, we only considered the number of training rounds as our resource budget. However, practical FL settings consider a wide variety of costs, such as total communication, amount of local training, or total time to train a model.
- **Combining proxy and client data for HP tuning.** A key issue of using public proxy data for HP tuning is that the best proxy dataset is not known in advance. One direction to address this is to design methods that combine public and private evaluations to mitigate bias from proxy data and noise from private data. Another promising direction is to rely on the abundance of public data and design a method that can select the best proxy dataset.

